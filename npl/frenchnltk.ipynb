{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://stackoverflow.com/questions/42058396/python-nltk-and-textblob-in-french\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from textblob import TextBlob\n",
    "\n",
    "jar = 'stanford-postagger-full-2020-11-17/stanford-postagger.jar'\n",
    "model = 'stanford-postagger-full-2020-11-17/models/french-ud.tagger'\n",
    "os.environ['JAVAHOME'] = '/usr/bin/java'\n",
    "\n",
    "\n",
    "# pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8' )\n",
    "# res = pos_tagger.tag(blob.split())\n",
    "# print(res)\n",
    "\n",
    "\n",
    "# import sentences from asset csv file\n",
    "with open('../asset/SpeechDestination.csv', 'r') as f:\n",
    "    sentences = f.read()\n",
    "\n",
    "# remove first line\n",
    "sentences = sentences.splitlines()[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Je', 'veux', 'aller', 'de', 'Marseille', 'à', 'Paris', '.'], ['Il', 'faudrait', 'que', \"j'aille\", 'en', 'vacance', 'à', 'Montpellier', 'mais', 'je', 'part', 'de', 'Brest', '.'], ['Je', 'vais', 'rejoindre', 'ma', 'famille', 'à', 'Nice', 'et', 'je', 'suis', 'actuellement', 'à', 'Toulouse', '.'], ['Je', 'vais', 'bien', 'mais', 'je', 'ne', 'sais', 'pas', 'quel', 'train', 'prendre', 'pour', 'aller', 'de', 'Montpellier', 'à', 'Marseille', '.'], ['Je', 'ne', 'sais', 'pas', 'ce', 'que', 'je', 'cherche', '.'], [\"C'est\", 'une', 'catastrophe', ',', 'mon', 'train', 'est', 'annulé', '.', 'Je', 'devais', 'faire', 'Toulouse', 'à', 'Montpellier', 'demain', 'soir', 'mais', 'je', 'ne', 'sais', 'pas', 'comment', 'faire', '.'], ['Comment', 'dois-je', 'm', \"'\", 'y', 'prendre', 'pour', 'aller', 'de', 'Lille', 'à', 'Carcassone', '?'], ['Quel', 'est', 'le', 'meilleur', 'itinéraire', 'pour', 'aller', 'de', 'Auxerre', 'à', 'Béziers', 'en', 'passant', 'par', 'Nantes', '?'], [\"j'aimerais\", 'partir', 'de', 'Paris', 'et', 'passer', 'par', 'biarritz', 'pour', 'me', 'rendre', 'à', 'Marseille', '.'], ['arrivée', 'Abbévillers', 'et', 'départ', 'Abondant'], ['Je', 'quitte', 'Abilly', 'pour', 'Ablon-sur-Seine'], [\"J'irai\", 'bien', 'à', 'la', 'gare', \"d'austerlitz\", 'depuis', 'la', 'gare', 'de', 'toulouse'], ['Départ', 'Bordeaux', 'arrivée', 'Nice'], ['arrivée', 'Paris', 'gare', 'de', 'lyon', 'en', 'partant', 'de', 'béziers'], ['Je', 'dois', 'aller', 'à', 'Toulon', '.', 'Mais', 'je', 'ne', 'sais', 'pas', 'comment', '.', 'Je', 'pars', 'de', 'Saint', 'Ouen', '.'], ['Montpellier', 'à', 'Strasbourg'], ['quel', 'est', 'le', 'trajet', 'le', 'plus', 'cours', 'de', 'La', 'Rochelle', 'vers', 'Cannes', '?'], ['Train', 'entre', 'Dijon', 'et', 'Perpignan'], ['Je', 'pars', 'en', 'vacance', 'à', 'Montpellier', '.', \"J'habite\", 'à', 'Pau'], ['Y', \"'\", 'a', 't-il', 'un', 'train', 'qui', 'va', 'de', 'Beziers', 'à', 'Montpellier', '?']]\n"
     ]
    }
   ],
   "source": [
    "# french_stopwords = set(stopwords.words('french'))\n",
    "# filtre_stopfr =  lambda text: [token for token in text if token.lower() not in french_stopwords]\n",
    "\n",
    "\n",
    "sentencesWords = []\n",
    "# tokenize sentences into words\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence, language=\"french\")\n",
    "    sentencesWords.append(words)\n",
    "\n",
    "print(sentencesWords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Je', 'PRON'), ('veux', 'VERB'), ('aller', 'VERB'), ('de', 'ADP'), ('Marseille', 'PROPN'), ('à', 'ADP'), ('Paris', 'PROPN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8' )\n",
    "res = pos_tagger.tag(sentencesWords[0])\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
